{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17ed4bf-14dd-4372-86c8-cc7e4f77e14f",
   "metadata": {},
   "source": [
    "# Experimental Evaluation Reproducibility Notebook \"Soft and Constrained Hypertree Width\"\n",
    "\n",
    "This JupyterLab notebook contains the exact same code that was used to run the experiments shown in the paper. \n",
    "\n",
    "## The Scala-based Rewriting Tool \n",
    "\n",
    "An important part of our implementation is the Scala-based Rewriting tool that takes a CTD and encodes it into a series of SQL queries that effectively implement Yannakakis' algorithm, adapted for CTDs. This tool is based off of code from (https://doi.org/10.34726/hss.2024.120310), which was adapted to fit the exact scenario in the paper. The source code for it can be found in the same repository in which this notebook was located. \n",
    "\n",
    "A user needs to compile and export our version of the Scala tool, as indicated in the ReadMe inside the Scala source code. This will produce a .jar file which needs to be put in the same folder as this notebook (for simplicity), and the name of which must be entered as indicated in the comments of the cells below. \n",
    "\n",
    "\n",
    "## Used benchmarks\n",
    "\n",
    "A crucial point that must be setup externally before these suite of experiments can be used are the respective benchmarks. The paper used the following datasets: \n",
    "* TPC-DS, using a scaling factor 10. TPC-DS can be found at its homepage: (https://www.tpc.org/tpcds/).\n",
    "* Hetionet, as can be found using the data files and queries from (https://github.com/umbra-db/diamond-vldb2024). Please note the FigShare link in the ReadMe of that link to find the actual data files.\n",
    "* LSQB, with a scaling factor of 10. All information on LSQB, and how to set it up, can be found in its respective paper: (https://dl.acm.org/doi/10.1145/3461837.3464516).\n",
    "\n",
    "Any user of this notebook is expected to setup the benchmarks on a local installation of Postgres, using the provided scale factors where applicable. Where needed, the name of the exact database, the user name and the password, must be inserted for the experiments to succeed.  The comments will indicate where these configuration strings need to be appropriately adjusted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccad4e-a48c-4357-9318-a15e68be174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install matplotlib\n",
    "pip3 install psycopg2-binary\n",
    "pip3 install networkx\n",
    "pip3 install colorama\n",
    "pip3 install termcolor\n",
    "pip3 install py4j\n",
    "pip3 install numba\n",
    "pip3 install pandas\n",
    "pip3 install line_profiler\n",
    "pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c176f2-3472-4700-b6de-0643b13812b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from functools import partial\n",
    "import sys\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from py4j.java_collections import SetConverter, MapConverter, ListConverter\n",
    "import heapq\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc65fd-63ac-4461-8ea4-c321b308279c",
   "metadata": {},
   "source": [
    "## Source Python Files for the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c9b76-77e9-4fcf-af41-60a1faccfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import re\n",
    "import pprint\n",
    "import itertools\n",
    "import colorama\n",
    "from termcolor import colored\n",
    "import functools\n",
    "colorama.init()\n",
    "\n",
    "class Edge(object):\n",
    "    def __init__(self,V,name):\n",
    "        assert(type(name) == str)\n",
    "        assert(type(V) == set)\n",
    "        self.V = V\n",
    "        self.name = name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name + \"(\" + \",\".join(map(str,self.V)) + \")\"\n",
    "\n",
    "class HyperGraph(object):\n",
    "    def __init__(self):\n",
    "        self.V = set()\n",
    "        self.E = list()\n",
    "        self.edge_dict = dict()\n",
    "\n",
    "\n",
    "    def markComplete(self):\n",
    "        newH = HyperGraph()\n",
    "        count = 1\n",
    "        for e in self.E:\n",
    "            newVertex = str(-1*count)\n",
    "            count = count+1\n",
    "            newV = e.V.copy()\n",
    "            newV.add(newVertex)\n",
    "            newH.add_edge(newV, e.name)\n",
    "        return newH\n",
    "        \n",
    "            \n",
    "\n",
    "    def grid(n, m):\n",
    "        h = HyperGraph()\n",
    "        hc, vc = 0, 0\n",
    "        for col in range(m-1):\n",
    "            for row in range(n):\n",
    "                vi = '{}.{}'.format(row, col)\n",
    "                vright = '{}.{}'.format(row, col+1)\n",
    "                horz_name = 'H{}'.format(hc)\n",
    "                hc = hc+1\n",
    "                h.add_edge(set([vi, vright]),\n",
    "                           horz_name)\n",
    "        for col in range(m):\n",
    "            for row in range(n-1):\n",
    "                vi = '{}.{}'.format(row, col)\n",
    "                vdown = '{}.{}'.format(row+1, col)\n",
    "                vert_name = 'V{}'.format(vc)\n",
    "                vc = vc+1\n",
    "                h.add_edge(set([vi, vdown]),\n",
    "                           vert_name)\n",
    "        return h\n",
    "\n",
    "    def copy(self):\n",
    "        h = HyperGraph()\n",
    "        for en, e in self.edge_dict.items():\n",
    "            h.add_edge(e.V, name=en)\n",
    "        return h\n",
    "\n",
    "    def join_copy(self, x, y):\n",
    "        \"\"\"Copy of self with vertices x and y joined\"\"\"\n",
    "        if x not in self.V or y not in self.V:\n",
    "            raise ValueError('Join vertices need to be in hypergraph')\n",
    "        h = HyperGraph()\n",
    "        for en, e in self.edge_dict.items():\n",
    "            e2 = e.V.copy()\n",
    "            if y in e2:\n",
    "                e2.remove(y)\n",
    "                e2.add(x)\n",
    "            h.add_edge(e2, name=en)\n",
    "        return h\n",
    "\n",
    "    def toHyperbench(self):\n",
    "        s = []\n",
    "        for en, e in sorted(self.edge_dict.items()):\n",
    "            s.append('{}({}),'.format(en, ','.join(e.V)))\n",
    "        return '\\n'.join(s)\n",
    "\n",
    "    def vertex_induced_subg(self, U):\n",
    "        \"\"\"Induced by vertex set U\"\"\"\n",
    "        h = HyperGraph()\n",
    "        for en, e in self.edge_dict.items():\n",
    "            e2 = e.V.copy()\n",
    "            e2 = e2 & U\n",
    "            if e2 != set():\n",
    "                h.add_edge(e2, name=en)\n",
    "        return h\n",
    "\n",
    "    def bridge_subg(self, U):\n",
    "        EC = [en for en, e in self.edge_dict.items() if\n",
    "              (e.V & U) != set()]\n",
    "        C = self.edge_subg(EC)\n",
    "\n",
    "        # for each component C_i of rest, compute a special edge Sp_i\n",
    "        for C_i in self.separate(U):\n",
    "            # print(C_i)\n",
    "            Sp_i_parts = [(e.V - U) for e in C.E if (e.V & C_i.V) != set()]\n",
    "            Sp_i = set.union(*Sp_i_parts)\n",
    "            C.add_special_edge(Sp_i)\n",
    "        return C\n",
    "\n",
    "    def edge_subg(self, edge_names):\n",
    "        h = HyperGraph()\n",
    "        for en in edge_names:\n",
    "            if en not in self.edge_dict:\n",
    "                raise ValueError('Edge >{}< not present in hypergraph'.format(en))\n",
    "            h.add_edge(self.edge_dict[en].copy(), en)\n",
    "        return h\n",
    "\n",
    "    def fromHyperbench(fname):\n",
    "        EDGE_RE = re.compile('\\s*([\\w:]+)\\s?\\(([^\\)]*)\\)')\n",
    "        def split_to_edge_statements(s):\n",
    "            x = re.compile('\\w+\\s*\\([^\\)]+\\)')\n",
    "            return list(x.findall(s))\n",
    "\n",
    "        def cleanup_lines(rl):\n",
    "            a = map(str.rstrip, rl)\n",
    "            b = filter(lambda x: not x.startswith('%') and len(x) > 0, a)\n",
    "            return split_to_edge_statements(''.join(b))\n",
    "\n",
    "        def line_to_edge(l):\n",
    "            m = EDGE_RE.match(l)\n",
    "            name = m.group(1)\n",
    "            e = m.group(2).split(',')\n",
    "            e = set(map(str.strip, e))\n",
    "            return name, e            \n",
    "\n",
    "        with open(fname) as f:\n",
    "            raw_lines = f.readlines()\n",
    "        lines = cleanup_lines(raw_lines)\n",
    "\n",
    "        hg = HyperGraph()\n",
    "        for l in lines:\n",
    "            edge_name, edge = line_to_edge(l)\n",
    "            hg.add_edge(edge, edge_name)\n",
    "        return hg\n",
    "\n",
    "    def add_edge(self, edge, name):\n",
    "        assert(type(edge) == set)\n",
    "        obj = Edge(edge,name)\n",
    "        self.edge_dict[name] = obj\n",
    "        self.V.update(edge)\n",
    "        self.E.append(obj)\n",
    "\n",
    "    def add_special_edge(self, sp):\n",
    "        SPECIAL_NAME = 'Special'\n",
    "        # find a name first\n",
    "        sp_name = None\n",
    "        for i in itertools.count():\n",
    "            candidate = SPECIAL_NAME + str(i)\n",
    "            if candidate not in self.edge_dict:\n",
    "                sp_name = candidate\n",
    "                break\n",
    "        self.add_edge(sp, sp_name)\n",
    "\n",
    "    def remove_edge(self, name):\n",
    "        e = self.edge_dict[name]\n",
    "        del self.edge_dict[name]\n",
    "        self.E.remove(e)\n",
    "\n",
    "    def primal_nx(self):\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.V)\n",
    "        for i, e in enumerate(self.E):\n",
    "            for a, b in itertools.combinations(e.V, 2):\n",
    "                G.add_edge(a, b)\n",
    "        return G\n",
    "\n",
    "    def incidence_nx(self, without=[]):\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.V)\n",
    "        G.add_nodes_from(self.edge_dict.keys())\n",
    "        for n, e in self.edge_dict.items():\n",
    "            if n in without:\n",
    "                continue\n",
    "            for v in e.V:\n",
    "                G.add_edge(n, v)\n",
    "        return G\n",
    "\n",
    "    def toPACE(self, special=[]):\n",
    "        buf = list()\n",
    "        vertex2int = {v: str(i) for i, v in enumerate(self.V, start=1)}\n",
    "        buf.append('p htd {} {}'.format(len(self.V),\n",
    "                                        len(self.E)))\n",
    "        for i, ei in enumerate(sorted(self.edge_dict.items()), start=1):\n",
    "            en, e = ei.V\n",
    "            edgestr = ' '.join(map(lambda v: vertex2int[v], e))\n",
    "            line = '{} {}'.format(i, edgestr)\n",
    "            buf.append(line)\n",
    "\n",
    "        if special is None:\n",
    "            special = []\n",
    "        for sp in special:\n",
    "            if sp is None:\n",
    "                continue\n",
    "            edgestr = ' '.join(map(lambda v: vertex2int[v], sp))\n",
    "            buf.append('s ' + edgestr)\n",
    "        return '\\n'.join(buf)\n",
    "\n",
    "    def separation_subg(self, U, sep):\n",
    "        C = HyperGraph()\n",
    "        cover = U | sep\n",
    "        for en, e in self.edge_dict.items():\n",
    "            if e.V.issubset(cover) and not e.V.issubset(sep):\n",
    "                C.add_edge(e.V, en)\n",
    "        return C\n",
    "\n",
    "    def separate(self, sep, only_vertices=False):\n",
    "        \"\"\"Returns list of components\"\"\"\n",
    "        assert(type(sep) == set)\n",
    "        primal = self.primal_nx()\n",
    "        primal.remove_nodes_from(sep)\n",
    "        comp_vertices = nx.connected_components(primal)\n",
    "        if only_vertices:\n",
    "            return list(comp_vertices)\n",
    "        comps = [self.separation_subg(U, sep)\n",
    "                 for U in comp_vertices]\n",
    "        return comps\n",
    "\n",
    "    def toVisualSC(self):\n",
    "        vertex2int = {v: str(i) for i, v in enumerate(self.V, start=1)}\n",
    "        edges = map(lambda e: map(lambda v: vertex2int[v], e.V), self.E)\n",
    "        buf = []\n",
    "        for e in edges:\n",
    "            buf.append('{'+', '.join(e) + '}')\n",
    "        return ' '.join(buf)\n",
    "\n",
    "    def fancy_repr(self, hl=[]):\n",
    "        edge_style = colorama.Fore.RED + colorama.Style.NORMAL\n",
    "        vertex_style = colorama.Fore.YELLOW + colorama.Style.NORMAL\n",
    "        hl_style = colorama.Fore.WHITE + colorama.Back.GREEN + colorama.Style.BRIGHT\n",
    "        _reset = colorama.Style.RESET_ALL\n",
    "\n",
    "        def color_vertex(v):\n",
    "            if v in hl:\n",
    "                return hl_style + v + _reset\n",
    "            else:\n",
    "                return vertex_style + v + _reset\n",
    "        s = ''\n",
    "        for en, e in sorted(self.edge_dict.items()):\n",
    "            s += edge_style + en + _reset + '('\n",
    "            s += ','.join(map(color_vertex, e.V))\n",
    "            s += ')\\n'\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.fancy_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ddfea-7ee7-492b-b7f7-35a1b9a060a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class VertSet(object):\n",
    "    def __init__(self,vertices):\n",
    "        assert(type(vertices) == set)\n",
    "        self.vertices = vertices\n",
    "\n",
    "         \n",
    "    def __hash__(self):\n",
    "        finalHash = 0 \n",
    "        for h in self.vertices:\n",
    "            finalHash = finalHash + int(h)\n",
    "        return finalHash   \n",
    "        \n",
    "    def __repr__(self):        \n",
    "        return str(sorted(list(map(lambda s: int(s), self.vertices))))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return type(other) == VertSet and self.vertices == other.vertices\n",
    "\n",
    "class Block(object):\n",
    "    def __init__(self,head,cover,tail):\n",
    "        assert(type(head) == VertSet)\n",
    "        assert(type(tail) == VertSet)\n",
    "        assert(len(head.vertices.intersection(tail.vertices)) == 0) # disjoint\n",
    "        self.head = head\n",
    "        self.cover = cover\n",
    "        self.tail = tail\n",
    "\n",
    "    def __hash__(self):\n",
    "        finalHash = 0 \n",
    "        for h in self.head.vertices:\n",
    "            finalHash = finalHash + hash(h)\n",
    "        for t in self.tail.vertices:\n",
    "            finalHash = finalHash + hash(t)\n",
    "        return finalHash\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return type(other) == Block and self.head == other.head and self.tail == other.tail\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return \"Block(\"+str(self.head)+\",\"+str(self.tail)+\",\"+str(self.cover)+\")\"\n",
    "\n",
    "\n",
    "    def __lt__(self,other):\n",
    "        selfVert = self.head.vertices.union(self.tail.vertices) \n",
    "        otherVert = other.head.vertices.union(other.tail.vertices) \n",
    "\n",
    "        return selfVert.issubset(otherVert) and self.tail.vertices.issubset(other.tail.vertices)\n",
    "\n",
    "    # connected bags filters out any bags for which the induced subgraph over E is not connected\n",
    "    def connected(self):\n",
    "        coverGraph = HyperGraph()\n",
    "\n",
    "        for e in self.cover: \n",
    "            coverGraph.add_edge(e.V,e.name)\n",
    "        comps = coverGraph.separate(set())\n",
    "        # if len(comps) == 1: \n",
    "        #     print(\"For the block \" + str(self)  + \" with cover \"+ str(self.cover)+ \" there are these components \" + str(comps) )\n",
    "      \n",
    "        return len(comps) == 1 # connected if only one connected comp\n",
    "\n",
    "    def index(self):\n",
    "        cover = list(map(lambda e: e.name, self.cover))\n",
    "        return \",\".join(sorted(cover))\n",
    "\n",
    "class Node:    \n",
    "    def __init__(self,bag,cover,children, weight = 0, weight_ideal = 0):\n",
    "        assert(type(bag) == VertSet)\n",
    "        self.bag = bag  # set of vertices\n",
    "        self.cover = cover #set of edges\n",
    "        self.children = children #set of child nodes\n",
    "        self.weight = weight\n",
    "        self.weight_ideal = weight_ideal\n",
    "\n",
    "\n",
    "    def getCoverWeight(self,node_weights):\n",
    "        nuCover = []\n",
    "        for e in self.cover:\n",
    "            nuCover.append(e.name)        \n",
    "        cover_index = \",\".join(sorted(nuCover))\n",
    "        return node_weights[cover_index]\n",
    "        \n",
    "\n",
    "    def NodeWeight(self,node_weights):\n",
    "        if len(self.cover) == 1: \n",
    "            return 1\n",
    "        else:\n",
    "            sumSingleEdgeWeights = 0\n",
    "            for e in self.cover:\n",
    "                subcover = [e.name]\n",
    "                subcover_index = \",\".join(sorted(subcover))\n",
    "                if node_weights[subcover_index] != 0:\n",
    "                    sumSingleEdgeWeights = sumSingleEdgeWeights + ( node_weights[subcover_index] * math.log(node_weights[subcover_index]))\n",
    "            return self.getCoverWeight(node_weights) + sumSingleEdgeWeights\n",
    "\n",
    "    def ReducedSz(self,node_weights):\n",
    "        # check if child has ReducedSz of 0\n",
    "        for c in self.children:\n",
    "            if c.ReducedSz(node_weights) <= 1:\n",
    "                return 1\n",
    "        return self.getCoverWeight(node_weights)\n",
    "\n",
    "\n",
    "    def ScanCost(self,node_weights): \n",
    "        #check if left-most child has ReducedSz of 0\n",
    "        if len(self.children) == 0:\n",
    "            return 0\n",
    "        if list(self.children)[0].ReducedSz(node_weights) <= 1: \n",
    "            return 0\n",
    "        if self.getCoverWeight(node_weights) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.getCoverWeight(node_weights) * math.log(self.getCoverWeight(node_weights))\n",
    "        \n",
    "        \n",
    "    def TotalWeight(self,node_weights):\n",
    "        nodeWeight = self.NodeWeight(node_weights)\n",
    "        self_join_costs = 0\n",
    "        for c in self.children: \n",
    "            child_node_weight, scan_costs, child_sj_costs = c.TotalWeight(node_weights)\n",
    "            child_weight = child_node_weight+child_sj_costs+scan_costs\n",
    "            if c.ReducedSz(node_weights) == 0:\n",
    "                self_join_costs = self_join_costs  + child_weight\n",
    "            else:\n",
    "                self_join_costs = self_join_costs  + child_weight + (c.ReducedSz(node_weights) * math.log(c.ReducedSz(node_weights)))\n",
    "        \n",
    "        return (nodeWeight,self.ScanCost(node_weights),self_join_costs)\n",
    "        \n",
    "\n",
    "    def removeMarkers(self):\n",
    "        badVs = []\n",
    "        for v in self.bag.vertices:\n",
    "            if int(v) < 0:\n",
    "                badVs.append(v)\n",
    "        for v in badVs:\n",
    "            self.bag.vertices.remove(v)\n",
    "        \n",
    "        for e in self.cover: \n",
    "            badVs = []\n",
    "            for v in e.V:\n",
    "                if int(v) < 0:\n",
    "                    badVs.append(v)\n",
    "            for v in badVs:\n",
    "                e.V.remove(v)\n",
    "        for c in self.children: \n",
    "            c.removeMarkers()\n",
    "    \n",
    "         \n",
    "    def addChild(self,child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    \n",
    "    def toStringCost(self,depth,node_weights):\n",
    "\n",
    "        node_weight,scan_cost,sj_costs = self.TotalWeight(node_weights)\n",
    "        \n",
    "        tabby = \"\\n \" + \"\\t\" * depth\n",
    "        \n",
    "        childrenReps = list()\n",
    "        for child in self.children:\n",
    "            childrenReps.append(child.toStringCost(depth+1,node_weights))\n",
    "\n",
    "        return \"Bag: \" + str(self.bag) + \" Cover: \" + str(self.cover) + \"Total Cost: \" + str(node_weight + sj_costs) +   \" NodeCost: \" + str(node_weight) +  \" ScanCost: \" + str(scan_cost) +   \"  SubTree Costs:\" + str(sj_costs) + tabby + tabby.join(childrenReps)\n",
    "\n",
    "        \n",
    "    def toString(self,depth):\n",
    "\n",
    "        tabby = \"\\n \" + \"\\t\" * depth\n",
    "        \n",
    "\n",
    "        childrenReps = list()\n",
    "        for child in self.children:\n",
    "            childrenReps.append(child.toString(depth+1))\n",
    "\n",
    "        return \"Bag: \" + str(self.bag) + \" Cover: \" + str(self.cover) + tabby + tabby.join(childrenReps)\n",
    "    \n",
    "    def __repr__(self):        \n",
    "        return self.toString(1)\n",
    "\n",
    "class NodeEncoder(JSONEncoder):\n",
    "    def default(self, o):\n",
    "        return {'bag': list(o.bag.vertices),\n",
    "                'cover': list([{'name': e.name, 'vertices': list(e.V)} for e in o.cover]),\n",
    "                'children': [self.default(c) for c in o.children]}\n",
    "\n",
    "@dataclass\n",
    "class WeightedBasis:\n",
    "    weight: int\n",
    "    weight_ideal: int\n",
    "    basis: Any\n",
    "    def __lt__(self, other):\n",
    "        return self.weight > other.weight\n",
    "\n",
    "\n",
    "class CTDOpt(object):\n",
    "    def __init__(self,h):\n",
    "        self.H = h                   # hypergraph\n",
    "        self.root_block = Block(VertSet(set()), set(), VertSet(h.V))\n",
    "        self.blocks = set([self.root_block])\n",
    "        self.satisfied_block = set() # indicating which blocks are satisfied\n",
    "        self.head_to_blocks = dict() # mapping heads to blocks headed by them\n",
    "        self.weights = dict() # maps block to weight\n",
    "        self.weights[self.root_block] = sys.maxsize\n",
    "        self.weights_ideal = dict()\n",
    "        self.weights_ideal[self.root_block] = sys.maxsize\n",
    "        self.sj_weights = dict()\n",
    "        self.children = dict()\n",
    "        self.top_children = dict()\n",
    "        self.top_children[self.root_block] = []\n",
    "        self.new_blocks = set()\n",
    "        self.head_to_cover = dict() # cache the edge covers\n",
    "        self.block_to_basis = dict() # mapping a satisfied block to its basis\n",
    "        self.rootHead = None # cache the root head once found\n",
    "\n",
    "    def addBlock(self,b):\n",
    "        assert(type(b) == Block)\n",
    "        if b in self.blocks:\n",
    "            return # don't add same block twice\n",
    "        self.blocks.add(b)\n",
    "        self.head_to_cover[b.head] = b.cover\n",
    "        self.new_blocks.add(b)\n",
    "        # print(\"Is the head \", b.head ,\" hash:\",hash(b.head)  ,\" already in the map \", list(self.head_to_blocks.keys()))\n",
    "        # print(\"Answer: \", b.head in list(self.head_to_blocks.keys()))\n",
    "        if b.head in self.head_to_blocks:\n",
    "            self.head_to_blocks[b.head].append(b)\n",
    "        else:\n",
    "            self.head_to_blocks[b.head] = [b]            \n",
    "\n",
    "        self.top_children[b] = []\n",
    "        if len(b.tail.vertices) == 0: \n",
    "            # print(\"Block \",b,\" added as trivially sat.\")\n",
    "            self.satisfied_block.add(b)  # check if trivially satisifed\n",
    "            block_index = b.index()\n",
    "            if block_index in self.node_weights:\n",
    "                self.weights[b] = self.node_weights[block_index]\n",
    "                self.weights_ideal[b] = self.node_weights_ideal[block_index]\n",
    "            else:\n",
    "                # single edge\n",
    "                self.weights[b] = 1\n",
    "                self.weights_ideal[b] = 1\n",
    "            self.children[b] = set()\n",
    "        else:\n",
    "            self.weights[b] = sys.maxsize\n",
    "            self.weights_ideal[b] = sys.maxsize\n",
    "        # else:\n",
    "        #     self.block_dict[b] = self.hasBasis(b) # basis check\n",
    "\n",
    "    def minimize_weights(self, topn):\n",
    "        # new_blocks = blocks that were updated in the last iteration -> continue until there are no more updates\n",
    "        while self.new_blocks != set():\n",
    "            new = set() # keep track of newly added blocks to stop when nothing new is added\n",
    "            for b in self.blocks:\n",
    "                # print(\"checking block \", b)\n",
    "                if len(b.tail.vertices) == 0:\n",
    "                    # skip trivial blocks\n",
    "                    continue\n",
    "                bases = self.determine_bases(b, self.new_blocks)\n",
    "                #print(\"bases: \" + str(bases))\n",
    "                all_children = self.top_children[b]\n",
    "                for basis in bases:\n",
    "                    basis.sort(key=self.reducedSz)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    # print(\"basis: \" + str(basis))\n",
    "                    new_weight = self.basis_weight(b, basis)\n",
    "                    new_weight_ideal = self.basis_weight_ideal(b, basis)\n",
    "                    # print(\"new weight: \" + str(new_weight) + \", old weight: \" + str(self.weights[b]))\n",
    "                    # basis_sum = sum(list(map(lambda b: self.weights[b], basis)))\n",
    "                    weight = self.weights[b]\n",
    "                    all_children.append(WeightedBasis(new_weight, new_weight_ideal, basis))\n",
    "                    if new_weight < weight:\n",
    "                        self.weights[b] = new_weight\n",
    "                        self.weights_ideal[b] = new_weight_ideal\n",
    "                        self.children[b] = basis\n",
    "                        self.block_to_basis[b] = basis\n",
    "                        new.add(b)\n",
    "                #print(\"all children: \", all_children)\n",
    "                self.top_children[b] = heapq.nlargest(topn, all_children)\n",
    "                #print(\"sorted\", self.top_children[b])\n",
    "            self.new_blocks = new\n",
    "        if self.weights[self.root_block] == sys.maxsize:\n",
    "            print(\"no decomposition found\")\n",
    "            return None\n",
    "        else:\n",
    "            decomps = self.construct_tds(topn)\n",
    "            # # decomps = [self.construct_td()]\n",
    "            # print(\"decompositions found: \")\n",
    "            # root_block = self.root_block\n",
    "            # root_basis = self.children[root_block] \n",
    "            # print(\"real optimal weight: \", self.basis_weight(root_block, root_basis), \"\\n\")\n",
    "            # for decomp in decomps:\n",
    "            #     print(decomp.toStringCost(1,self.node_weights))\n",
    "            #     print(\"weight: \", decomp.weight, \"\\n\")\n",
    "            # print(\"root block children\", self.children[self.root_block])\n",
    "            return decomps\n",
    "\n",
    "    def construct_td(self):\n",
    "        return self.to_node(self.root_block)\n",
    "    \n",
    "    def construct_tds(self, topn):\n",
    "        return self.to_nodes(self.root_block, topn)\n",
    "\n",
    "    def add_weights(self, node_costs):\n",
    "        self.node_weights = node_costs\n",
    "\n",
    "    \n",
    "    def add_weights_ideal(self, node_costs):\n",
    "        self.node_weights_ideal = node_costs\n",
    "        \n",
    "    def add_sj_weights(self, sj_weights):\n",
    "        self.sj_weights = sj_weights\n",
    "\n",
    "    def block_weight(self, block):\n",
    "        cover = list(map(lambda e: e.name, block.cover))\n",
    "        block_index = \",\".join(sorted(cover))\n",
    "        if block_index in self.node_weights_ideal:\n",
    "            return self.node_weights_ideal[block_index]\n",
    "        else:\n",
    "            return 1\n",
    "            \n",
    "    def sj_weight(self, from_b, to_b):\n",
    "        cover_from = [e.name for e in from_b.cover]\n",
    "        idx_from = \",\".join(sorted(cover_from))\n",
    "        cover_to = [e.name for e in to_b.cover]\n",
    "        idx_to = \",\".join(sorted(cover_to))\n",
    "        idx = idx_from + \"-\" + idx_to\n",
    "\n",
    "        if idx in self.sj_weights:\n",
    "            return self.sj_weights[idx]\n",
    "        else:\n",
    "            return 10000000\n",
    "\n",
    "\n",
    "\n",
    "    def getCoverWeight(self,cover):\n",
    "        nuCover = []\n",
    "        for e in cover:\n",
    "            nuCover.append(e.name)        \n",
    "        cover_index = \",\".join(sorted(nuCover))\n",
    "        return self.node_weights[cover_index]\n",
    "        \n",
    "\n",
    "    def get_node_weight(self,cover):\n",
    "        if len(cover) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            J_u = self.getCoverWeight(cover)\n",
    "            sumSingleEdgeWeights = 0\n",
    "            for e in cover:\n",
    "                subcover = [e.name]\n",
    "                subcover_index = \",\".join(sorted(subcover))\n",
    "                if self.node_weights[subcover_index] != 0:\n",
    "                    sumSingleEdgeWeights = sumSingleEdgeWeights + (self.node_weights[subcover_index] * math.log(self.node_weights[subcover_index]))\n",
    "            return J_u + sumSingleEdgeWeights\n",
    "\n",
    "    #Note that ReducedAttr is assumed to be 0 here\n",
    "    def reducedSz(self,block):\n",
    "        if len(block.tail.vertices) == 0:\n",
    "            return -1\n",
    "        \n",
    "        basis = self.children[block]   \n",
    "        oneBasisBlock = next(iter(basis))\n",
    "        basis_head = oneBasisBlock.head\n",
    "        cover = self.head_to_cover[basis_head]\n",
    "        \n",
    "        # check if child of block has child with ReduceSz of 0\n",
    "        for c in basis: \n",
    "            if len(c.tail.vertices) == 0:\n",
    "                continue # trivial blocks are not nodes, hence ignored\n",
    "            else:\n",
    "                if self.reducedSz(c) <= 1:\n",
    "                    return 1 # not 0, to encourage smaller TDs\n",
    "        return  self.getCoverWeight(cover)\n",
    "\n",
    "    def scan_cost(self,block):\n",
    "        if len(block.tail.vertices) == 0:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "        basis = self.children[block]   \n",
    "        oneBasisBlock = next(iter(basis))\n",
    "        basis_head = oneBasisBlock.head\n",
    "        cover = self.head_to_cover[basis_head]\n",
    "        \n",
    "        \n",
    "        basis = self.children[block]   \n",
    "\n",
    "        if self.reducedSz(basis[0]) <= 1:\n",
    "            return 0\n",
    "\n",
    "        if self.getCoverWeight(cover) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.getCoverWeight(cover) * math.log(self.getCoverWeight(cover))\n",
    "        \n",
    "  \n",
    "      \n",
    "        \n",
    "    def basis_weight(self, block, basis):\n",
    "        if len(block.tail.vertices) == 0: # base case\n",
    "            return 0\n",
    "        else: \n",
    "            # get weight of node\n",
    "            oneBasisBlock = next(iter(basis))\n",
    "            head = oneBasisBlock.head # can safely assume that basis of non-trivial block is non-empty\n",
    "            root_cover = self.head_to_cover[head]\n",
    "            current_node_weight = self.get_node_weight(root_cover)\n",
    "            sj_costs = 0 \n",
    "            # children = []\n",
    "            for b in basis:\n",
    "                if len(b.tail.vertices) == 0:\n",
    "                    continue # trivial nodes do not contribute to weight\n",
    "                else: \n",
    "                    child_basis = self.children[b]                      \n",
    "\n",
    "                    child_cost = self.basis_weight(b,child_basis) # we can assume that any block in a basis also has a basis (or is trivial)\n",
    "                    if  self.reducedSz(b) == 0:\n",
    "                        sj_costs = sj_costs + child_cost\n",
    "                    else :\n",
    "                        sj_costs = sj_costs + child_cost + self.reducedSz(b) * math.log( self.reducedSz(b) ) \n",
    "                    # children.append(b)\n",
    "                    \n",
    "            # print(\"for cover: \", root_cover, \" Node Cost \", current_node_weight, \" ST Cost: \", sj_costs)\n",
    "            # print(\" Children: \", children)\n",
    "            # for c in children:\n",
    "            #     print(\"ReduzedSz of \", c, \" = \", self.reducedSz(c))\n",
    "            return current_node_weight + self.scan_cost(b) +  sj_costs\n",
    "        \n",
    "    def basis_weight_ideal(self, block, basis):\n",
    "        basis_sum = sum(list(map(lambda b: 0 if len(b.tail.vertices) == 0 else self.weights_ideal[b], basis)))\n",
    "        sj_costs = 0\n",
    "        for b in basis:\n",
    "            child_basis = self.children[b]\n",
    "            if len(child_basis) > 0:\n",
    "                child_block = next(iter(child_basis))\n",
    "                sj_costs += self.sj_weight(child_block, b)\n",
    "        return self.block_weight(next(iter(basis))) + sj_costs + basis_sum\n",
    "        # return sj_costs + basis_sum\n",
    "\n",
    "    # determine the bases of a block wrt. new blocks (one of the blocks has to be from new_blocks)\n",
    "    # a basis is a set of blocks\n",
    "    def determine_bases(self, b, new_blocks):\n",
    "        bases = []\n",
    "        #print(\"block: \" + str(b))\n",
    "        for head in self.head_to_blocks:\n",
    "            #print(\"head: \" + str(head))\n",
    "            allBlocks = self.head_to_blocks[head]\n",
    "            #print(\"allblocks: \" + str(allBlocks))\n",
    "            #headed_blocks = [x for x in allBlocks if x < b and not (x.head == b.head and x.tail == b.tail)]\n",
    "            headed_blocks = [x for x in allBlocks if x < b and not (x.head == b.head and x.tail == b.tail)]\n",
    "\n",
    "            #print(\"headed blocks: \" + str(headed_blocks))\n",
    "\n",
    "            if set(headed_blocks).intersection(new_blocks) == set():\n",
    "                continue\n",
    "\n",
    "            for ob in headed_blocks:\n",
    "                if self.weights[ob] == sys.maxsize:\n",
    "                    continue\n",
    "\n",
    "            # 3. condition (for each component C_i', the block (B', C_i') is satisfied\n",
    "            cond3 = True\n",
    "            for ob in headed_blocks:\n",
    "                if not ob in self.satisfied_block:\n",
    "                    cond3 = False\n",
    "            if cond3 == False:\n",
    "                #print(\"cond3 broken\")\n",
    "                continue #3nd Condition violated (testing first for efficiency)\n",
    "\n",
    "            # 1. condition (the tail of the block b is a subset of the union of\n",
    "            # the tails and the head\n",
    "            unionTails = set()\n",
    "            # union of the tails' vertices\n",
    "            for ob in headed_blocks:\n",
    "                for v in ob.tail.vertices:\n",
    "                    unionTails.add(v)\n",
    "            # add the head's vertices\n",
    "            for v in head.vertices:\n",
    "                unionTails.add(v)\n",
    "            if not b.tail.vertices.issubset(unionTails):\n",
    "                #print(\"cond1 broken\")\n",
    "                continue # 1st Condition violated\n",
    "\n",
    "            # 2. condition (each hyperedge partially contained in the tail of b has to be contained\n",
    "            # in the union of the tails and the head)\n",
    "            cond2 = True\n",
    "            for e in self.H.E:\n",
    "                if len(e.V.intersection(b.tail.vertices)) == 0:\n",
    "                    continue # find other edge\n",
    "                if not e.V.issubset(unionTails):\n",
    "                    cond2 = False\n",
    "                    #print(\"cond2 broken\")\n",
    "                    break\n",
    "            if cond2 == False:\n",
    "                continue # 2nd Condition violated\n",
    "\n",
    "            # basis found!\n",
    "            basis = headed_blocks\n",
    "            # for ob in headed_blocks:\n",
    "            #     basis.append(ob)\n",
    "            bases.append(basis)\n",
    "            #print(\"bases: \" + str(bases))\n",
    "        if bases != []:\n",
    "            self.satisfied_block.add(b)\n",
    "        return bases\n",
    "\n",
    "    def hasBasis(self,b):\n",
    "        basisFound = False\n",
    "        basisWitness = None\n",
    "        for B in self.head_to_blocks:\n",
    "            allBlocks = self.head_to_blocks[B]\n",
    "            blocks = [x for x in allBlocks if x < b]\n",
    "\n",
    "            cond3 = True\n",
    "            for ob in blocks:\n",
    "                if not ob in self.satisfied_block:\n",
    "                    cond3 = False\n",
    "            if cond3 == False:\n",
    "                continue #3nd Condition violated (testing first for efficiency)\n",
    "\n",
    "            unionTails = set()\n",
    "            for ob in blocks:\n",
    "                for v in ob.tail.vertices:\n",
    "                    unionTails.add(v)\n",
    "            for v in B.vertices:\n",
    "                unionTails.add(v)\n",
    "            if not  b.tail.vertices.issubset(unionTails):\n",
    "                continue # 1st Condition violated\n",
    "            cond2 = True\n",
    "            for e in self.H.E:\n",
    "                if len(e.V.intersection(b.tail.vertices)) == 0:\n",
    "                    continue # find other edge\n",
    "                if not e.V.issubset(unionTails):\n",
    "                    cond2 = False\n",
    "                    break\n",
    "            if cond2 == False:\n",
    "                continue # 2nd Condition violated\n",
    "            basisFound = True\n",
    "            basisWitness = B\n",
    "            # print(\"The basis of \", b , \" is \", B)\n",
    "            # print(\"The blocks headed by \", B)\n",
    "            # for BB in blocks:\n",
    "            #     print(str(BB)+\"\\n\")\n",
    "\n",
    "            break\n",
    "        if basisFound == True:\n",
    "            self.satisfied_block.add(b)\n",
    "            self.block_to_basis[b] = basisWitness\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def rootHeadFound(self):\n",
    "        for head in self.head_to_blocks:\n",
    "            blocks = self.head_to_blocks[head]\n",
    "            allSatisfied = True\n",
    "            for b in blocks:\n",
    "                if not b in self.satisfied_block:\n",
    "                    allSatisfied = False\n",
    "            if allSatisfied == True:\n",
    "                # print(\"Root Head is \",head)\n",
    "                self.rootHead = head\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def hasDecomp(self):\n",
    "        while True:            \n",
    "            changed = False\n",
    "            for b in self.blocks:\n",
    "                if b in self.satisfied_block:\n",
    "                    continue # already marked as satisfied\n",
    "                res = self.hasBasis(b)\n",
    "                if res == True:\n",
    "                    changed = True\n",
    "                    #print(\"Found basis for the block \", b)\n",
    "                if self.rootHeadFound():\n",
    "                    # print(\"Found decomp!\")\n",
    "                    return True\n",
    "            if changed == False:\n",
    "                # print(\"Nothing has changed anymore, terminating\")\n",
    "                return False\n",
    "\n",
    "    def to_node(self,block):\n",
    "        if not(block in self.satisfied_block):\n",
    "            # print(block, \" is not satisfied\")\n",
    "            return None  # Nothing to return if block not satisfied\n",
    "        if len(block.tail.vertices) == 0:\n",
    "            return Node(block.head,self.head_to_cover[block.head],list()) # leaf node\n",
    "        basis = self.block_to_basis[block]\n",
    "\n",
    "        node_children = list()\n",
    "        for block_child in self.children[block]:\n",
    "            if len(block_child.tail.vertices) != 0:\n",
    "                node_children.append(self.to_node(block_child))\n",
    "\n",
    "        basis_head = basis[0].head\n",
    "        return Node(basis_head,self.head_to_cover[basis_head],node_children)\n",
    "\n",
    "    def to_nodes(self,block,topn):\n",
    "        if not(block in self.satisfied_block):\n",
    "            # print(block, \" is not satisfied\")\n",
    "            return None  # Nothing to return if block not satisfied\n",
    "        if len(block.tail.vertices) == 0:\n",
    "            # print(block, \" is trivial\")\n",
    "            return Node(block.head,self.head_to_cover[block.head],list()) # leaf node\n",
    "\n",
    "        #print(\"top basis\", self.children[block])\n",
    "        #print(\"weight: \", self.weights[block])\n",
    "        # print(\"top children: \", self.top_children[block])\n",
    "        nodes = []\n",
    "        for weighted_basis in self.top_children[block]:\n",
    "            basis = weighted_basis.basis\n",
    "            node_children = list()\n",
    "            for block_child in weighted_basis.basis:\n",
    "                if len(block_child.tail.vertices) != 0:\n",
    "                    node_children.append(self.to_node(block_child))\n",
    "\n",
    "            basis_head = list(basis)[0].head\n",
    "\n",
    "            node_children.sort(key=lambda n : n.ReducedSz(self.node_weights))\n",
    "            \n",
    "            nodes.append(Node(basis_head,self.head_to_cover[basis_head],node_children, weighted_basis.weight, weighted_basis.weight_ideal))\n",
    "        return nodes\n",
    "\n",
    "    def getDecomp(self,block):\n",
    "        if not(block in self.satisfied_block):\n",
    "            # print(block, \" is not satisfied\")\n",
    "            return None  # Nothing to return if block not satisfied\n",
    "        if len(block.tail.vertices) == 0:\n",
    "            # print(block, \" is trivial\")\n",
    "            return Node(block.head,self.head_to_cover[block.head],list()) # leaf node\n",
    "        basis = self.block_to_basis[block]\n",
    "        allBlocks = self.head_to_blocks[basis]\n",
    "        blocks = [x for x in allBlocks if x < block]\n",
    "\n",
    "        # print(\"Child BLocks for block \", block)\n",
    "        # for bs in allBlocks:\n",
    "        #     print(bs)\n",
    "\n",
    "        children = list()\n",
    "        for bs in blocks: \n",
    "            children.append(self.getDecomp(bs))\n",
    "        \n",
    "        return Node(basis,self.head_to_cover[basis],children)\n",
    "\n",
    "\n",
    "    def getDecompRoot(self):\n",
    "        if self.rootHead == None:\n",
    "            return None  ## can't find decomp of whole graph if no root head\n",
    "\n",
    "        allBlocks = self.head_to_blocks[self.rootHead]\n",
    "        # print(\"Blocks of RootHead\")\n",
    "        # for bs in allBlocks:\n",
    "        #     print(bs)\n",
    "\n",
    "\n",
    "        blocks = [x for x in allBlocks if len(x.tail.vertices) != 0]\n",
    "\n",
    "        # print(\"Non-Trivial Blocks of RootHead\")\n",
    "        # for bs in blocks:\n",
    "        #     print(bs)\n",
    "\n",
    "        children = list()\n",
    "        for bs in blocks:\n",
    "            children.append(self.getDecomp(bs))\n",
    "\n",
    "        return Node(self.rootHead,self.head_to_cover[self.rootHead],children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147f0ff-21f0-4510-9f43-136f6dabc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def all_choose_k(S, k):\n",
    "    from itertools import chain, combinations\n",
    "    return chain(*(combinations(S, kp) for kp in range(1,k+1)))\n",
    "#\n",
    "def all_lambdas(E, k):\n",
    "    yield set()\n",
    "    for es in all_choose_k(E, k):\n",
    "        # yield set.union(*es)\n",
    "        yield es\n",
    "\n",
    "# (over approximates) the bags produced by the LogK algorithm\n",
    "def computesoftk(h, k):\n",
    "    softk = list()\n",
    "    lambdas = all_lambdas(h.E,k)\n",
    "    for P in lambdas:\n",
    "        obj1 = set()\n",
    "        if len(P) == 1:\n",
    "            obj1 = P[0].V\n",
    "        elif len(P) > 1:\n",
    "            obj1 =functools.reduce(lambda a,b: (a).union(b),map(lambda s : s.V,P))\n",
    "        for C in h.separate(obj1, only_vertices=False):\n",
    "            for L in lambdas:\n",
    "                obj2 = set()\n",
    "                if len(L) == 1:\n",
    "                    obj2 = L[0].V\n",
    "                elif len(L) > 1:\n",
    "                    obj2 = functools.reduce(lambda a,b: (a).union(b),map(lambda s : s.V,L))\n",
    "                B = set.intersection(C.V, obj2)\n",
    "                if len(B) > 1 and (B, L) not in softk:\n",
    "                    softk.append((B,L))\n",
    "    return softk\n",
    "    \n",
    "# computes the blocks of a bag by computing its components w.r.t. h\n",
    "def bag_to_blocks(h,pair):\n",
    "    B = pair[0]\n",
    "    L = pair[1]  \n",
    "    blocks = list()\n",
    "    for C in h.separate(B, only_vertices=True):\n",
    "        blocks.append(Block(VertSet(B),L,VertSet(C)))\n",
    "    blocks.append(Block(VertSet(B),L,VertSet(set())))  # adding trivial block too\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# connected lambda a,h : a.connected(h)\n",
    "def bag_to_blocks_constraint(h,constraint, pair): \n",
    "    # print(\"pair: \", pair)\n",
    "    blocksPrefilter = bag_to_blocks(h,pair)\n",
    "    return filter(constraint,blocksPrefilter)\n",
    "    # return blocksPrefilter\n",
    "\n",
    "\n",
    "# Same as  computeosftK, but returns directly the blocks\n",
    "def computesoftkBlocks(h, k):\n",
    "    out = list()\n",
    "    listOfLists = map(partial(bag_to_blocks,h),computesoftk(h,k))\n",
    "    for ll in listOfLists:\n",
    "        for l in ll:\n",
    "            out.append(l)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Same as  computeosftK, but returns directly the blocks\n",
    "def computesoftkBlocksConstraint(h, k,constraint):\n",
    "    out = list()\n",
    "    listOfLists = map(partial(bag_to_blocks_constraint,h,constraint),computesoftk(h,k))\n",
    "    for ll in listOfLists:\n",
    "        for l in ll:\n",
    "            out.append(l)\n",
    "    return out\n",
    "\n",
    "def get_best_blocks(blocks, node_to_cost):\n",
    "    best_weight = dict()\n",
    "    best_cover = dict()\n",
    "\n",
    "    for b in blocks:\n",
    "        idx = b.index()\n",
    "        weight = 1\n",
    "        if idx in node_to_cost:\n",
    "            weight = node_to_cost[idx]\n",
    "        cover = b.cover\n",
    "        if not b in best_weight:\n",
    "            best_weight[b] = weight\n",
    "            best_cover[b] = cover\n",
    "        else:\n",
    "            if weight < best_weight[b]:\n",
    "                best_weight[b] = weight\n",
    "                best_cover[b] = cover\n",
    "    new_blocks = []\n",
    "    for b, cover in best_cover.items():\n",
    "        new_blocks.append(Block(b.head, cover, b.tail))\n",
    "    return new_blocks\n",
    "\n",
    "def covers_intersect(cover1, cover2):\n",
    "    atts1 = set()\n",
    "    for e in cover1:\n",
    "        atts1.update(e.V)\n",
    "    for e in cover2:\n",
    "        if set.intersection(atts1, e.V) != set():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_connected_semijoins(covers):\n",
    "    all_sjs = list(itertools.permutations(covers, 2))\n",
    "\n",
    "    return [sj for sj in all_sjs if covers_intersect(sj[0], sj[1])]\n",
    "\n",
    "    \n",
    "def get_connected_semijoinsAll(covers):\n",
    "    return list(itertools.permutations(covers, 2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170bf854-27d4-4bcf-8f09-cab7ec6b54b9",
   "metadata": {},
   "source": [
    "## INSERT LOCATION OF COMPILED JAR FILE IN VAR BELOW!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba964037-c211-464b-a57e-bc98a50de1bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##### INSERT JAR FILE NAME HERE !!! ==================\n",
    "REWRITE_JAR = 'rewrite-assembly-0.1.0-SNAPSHOT.jar'\n",
    "##### INSERT JAR FILE NAME HERE !!! ==================\n",
    "\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import threading\n",
    "\n",
    "\n",
    "class Rewriting:\n",
    "    def __init__(self, original, rewritten, features, time, drop_statements):\n",
    "        self.original = original\n",
    "        self.rewritten = rewritten\n",
    "        self.features = features\n",
    "        self.time = time\n",
    "        self.drop_statements = drop_statements\n",
    "\n",
    "\n",
    "\n",
    "class QueryRewriter:\n",
    "    def __init__(self, host, database, user, password, port=5432, start_process=True):\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.port = port\n",
    "        self.jdbcString = f'jdbc:postgresql://{self.host}:{self.port}/{self.database}'\n",
    "        self.node_to_cost = dict()        \n",
    "        self.node_to_cost_ideal = dict()\n",
    "\n",
    "        if start_process:\n",
    "            self.rewrite_process = subprocess.Popen(['java', '-jar', REWRITE_JAR], stdout=subprocess.PIPE)\n",
    "            # Wait for the first line which is printed after the py4j server is started)\n",
    "            line = self.rewrite_process.stdout.readline()\n",
    "            # print(line)\n",
    "\n",
    "        self.gateway = JavaGateway()\n",
    "\n",
    "        self.rewriter = self.gateway.entry_point\n",
    "        self.rewriter.connect(self.jdbcString, self.database, self.user, self.password)\n",
    "\n",
    "\n",
    "    def rewrite_check_soft_numbers(self, query, k = 2, topn = 1):\n",
    "        SizeSoftSet = 0\n",
    "        \n",
    "        self.rewriter.connect(self.jdbcString, self.database, self.user, self.password)\n",
    "        self.rewriter.rewrite(query)\n",
    "\n",
    "        output = json.loads(Path('output/output.json').read_text())\n",
    "        drop_output = json.loads(Path('output/drop.json').read_text())\n",
    "\n",
    "        result = Rewriting(query, output['rewritten_query'], output['features'], output['time'],\n",
    "                           drop_output['rewritten_query'])\n",
    "\n",
    "        hg = HyperGraph.fromHyperbench('output/hypergraph.txt')\n",
    "\n",
    "        acyclic = output['acyclic']\n",
    "\n",
    "        if acyclic == True:\n",
    "            print(\"query is acyclic. done\")\n",
    "            for line in output[\"rewritten_query\"]:\n",
    "                print(line + \";\")\n",
    "            return \"acyclic\"\n",
    "        \n",
    "\n",
    "        blocksAll = computesoftkBlocks(hg,k)\n",
    "        \n",
    "        blocks = computesoftkBlocksConstraint(hg,k, lambda b: b.connected())\n",
    "\n",
    "        allDict = dict()\n",
    "\n",
    "        for b in blocksAll: \n",
    "            if b.head in allDict:\n",
    "                continue\n",
    "            allDict[b.head] = 0\n",
    "\n",
    "        conDict = dict()\n",
    "\n",
    "        for b in blocks: \n",
    "            if b.head in conDict:\n",
    "                continue\n",
    "            conDict[b.head] = 0\n",
    "        \n",
    "       \n",
    "        return (len(allDict.keys()),len(conDict.keys()),len(hg.E))\n",
    "\n",
    "    def rewrite(self, query, k = 2, topn = 1, quitAfterComputingTDs= False, Connected=False):):\n",
    "        \n",
    "        self.rewriter.connect(self.jdbcString, self.database, self.user, self.password)\n",
    "        self.rewriter.rewrite(query)\n",
    "\n",
    "        output = json.loads(Path('output/output.json').read_text())\n",
    "        drop_output = json.loads(Path('output/drop.json').read_text())\n",
    "\n",
    "        result = Rewriting(query, output['rewritten_query'], output['features'], output['time'],\n",
    "                           drop_output['rewritten_query'])\n",
    "\n",
    "        hg = HyperGraph.fromHyperbench('output/hypergraph.txt')\n",
    "\n",
    "        acyclic = output['acyclic']\n",
    "\n",
    "        if acyclic == True:\n",
    "            print(\"query is acyclic. done\")\n",
    "            for line in output[\"rewritten_query\"]:\n",
    "                print(line + \";\")\n",
    "            return \"acyclic\"\n",
    "        \n",
    "        print('hg: ' + str(hg.markComplete() ) )\n",
    "        hg = hg.markComplete()\n",
    "        ctd = CTDOpt(hg)\n",
    "\n",
    "\n",
    "        if Connected:\n",
    "            blocks = computesoftkBlocksConstraint(hg,k, lambda b: b.connected())\n",
    "        else:\n",
    "            blocks = computesoftkBlocks(hg,k)\n",
    "        \n",
    "\n",
    "        # keep only distinct covers\n",
    "        covers_dict = dict()\n",
    "        for b in blocks:\n",
    "            cover = list(map(lambda e: e.name, b.cover))\n",
    "            index = \",\".join(sorted(cover))\n",
    "            # ignore single edges\n",
    "            if not index in covers_dict:\n",
    "                covers_dict[index] = b.cover\n",
    "\n",
    "        # Get covers\n",
    "        candidate_covers = [[e.name for e in cover] for cover in covers_dict.values() if len(cover) > 1]\n",
    "        single_edge_covers = [[e.name] for e in hg.E if e not in candidate_covers]\n",
    "        candidate_covers = candidate_covers + single_edge_covers\n",
    "\n",
    "        #using SQL to extract cardinalities of bags and relations\n",
    "        node_cost_stats = json.loads(self.rewriter.determineNodeWeightsJSON(json.dumps(candidate_covers)))\n",
    "       \n",
    "        for (cover, extracted_cardinality) in node_cost_stats:\n",
    "            index = \",\".join(sorted(cover))\n",
    "            self.node_to_cost[index] = int(extracted_cardinality)\n",
    "        print(\"node costs: \" + str(self.node_to_cost))\n",
    "\n",
    "        #using the older 'idealised' costs, based on EXPLAIN queries\n",
    "        node_explain_plans_ideal = json.loads(self.rewriter.determineNodeWeightsJSONIdeal(json.dumps(candidate_covers)))\n",
    "       \n",
    "        for (cover, explain) in node_explain_plans_ideal:\n",
    "            index = \",\".join(sorted(cover))\n",
    "            plan = json.loads(explain)[0]\n",
    "            cost = plan['Plan']['Plan Rows']\n",
    "            self.node_to_cost_ideal[index] = cost\n",
    "        print(\"node costs ideal: \" + str(self.node_to_cost_ideal))\n",
    "\n",
    "        semijoins = get_connected_semijoins(covers_dict.values())\n",
    "        # keep only edge names (no vertices)\n",
    "        semijoins = [([e.name for e in cover1], [e.name for e in cover2]) for (cover1, cover2) in semijoins]\n",
    "\n",
    "        print(\"Getting sj costs from Java\")\n",
    "\n",
    "        #extracting SemiJoin costs via EXPLAIN queries, used as part of the 'idealised' cost\n",
    "        sj_explain_plans = json.loads(self.rewriter.determineSemijoinWeightsJSON(json.dumps(semijoins)))\n",
    "        print(\"Received sj costs from Java\")\n",
    "        \n",
    "        sj_to_cost = dict()\n",
    "        for (c1, c2, explain) in sj_explain_plans:\n",
    "            # print(\"explain \",explain)\n",
    "            index1 = \",\".join(sorted(c1))\n",
    "            index2 = \",\".join(sorted(c2))\n",
    "            index = index1 + \"-\" + index2\n",
    "            plan = json.loads(explain)[0]\n",
    "            sj_cost = plan['Plan']['Total Cost']\n",
    "            node_costs = self.node_to_cost_ideal.get(index1, 0) + self.node_to_cost_ideal.get(index2, 0)\n",
    "            sj_to_cost[index] = max(sj_cost - node_costs, 1)\n",
    "        print(\"sj costs: \" + str(sj_to_cost))\n",
    "\n",
    "        #print(\"\\n\".join(map(lambda b: str(b), blocks)))\n",
    "        ctd.add_weights(self.node_to_cost)\n",
    "        ctd.add_weights_ideal(self.node_to_cost_ideal)\n",
    "        ctd.add_sj_weights(sj_to_cost)\n",
    "        blocks = get_best_blocks(blocks, self.node_to_cost)\n",
    "        \n",
    "        \n",
    "        # print(\"\\n\".join(map(lambda b: str(b), blocks)))\n",
    "        # The weights have to be added before the blocks because the join costs for trivially satisfied blocks\n",
    "        # are set in addBlock\n",
    "        for b in blocks:\n",
    "            ctd.addBlock(b)\n",
    "        \n",
    "        \n",
    "        start_time_TD_comp = timer()\n",
    "        res = ctd.minimize_weights(topn)        \n",
    "        elapsed_time_TD_comp = timer() - start_time_TD_comp\n",
    "\n",
    "        print(\"Time it took to compute TDs: \", elapsed_time_TD_comp,  \"  len res set\" , len(res))\n",
    "\n",
    "        \n",
    "        # print(\"Result: \",res)\n",
    "\n",
    "        rewritings = []\n",
    "        drops = []\n",
    "        tds = []\n",
    "\n",
    "        if quitAfterComputingTDs:\n",
    "            return (rewritings, drops, tds)\n",
    "\n",
    "        for td in res:\n",
    "            if k > 1 and len(td.cover) == 1:\n",
    "                continue #skip TD whose root has only a single edge cover\n",
    "            \n",
    "            # print(\"---------------\\nTD as JSON\\n------------------\\n\")\n",
    "            # print(json.dumps(td, cls=NodeEncoder))\n",
    "\n",
    "            # print(\"original TD \", td)\n",
    "            td.removeMarkers()\n",
    "            # print(\"Cleaned Up TD: \",td)\n",
    "            \n",
    "            output = self.rewriter.rewriteCyclicJSON(json.dumps(td, cls=NodeEncoder))\n",
    "            output = json.loads(Path('output/output.json').read_text())\n",
    "            # print(\"-------------\\n OUTPUTTT \\n -----------\")\n",
    "            # print(output)\n",
    "            rewritings.append(output[\"rewritten_query\"])\n",
    "            # for line in output[\"rewritten_query\"]:\n",
    "            #     print(line + \";\")\n",
    "            drop_output = json.loads(Path('output/drop.json').read_text())\n",
    "            drops.append(drop_output[\"rewritten_query\"])\n",
    "            tds.append(td)\n",
    "\n",
    "        return (rewritings, drops, tds)\n",
    "\n",
    "    def run_rewriting(self, rewriting, drop_statements):\n",
    "            \n",
    "        conn = psycopg2.connect(\n",
    "            host=self.host,\n",
    "            database = self.database,\n",
    "            user = self.user,\n",
    "            password = self.password\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "                    \n",
    "        cur.execute(\"SET statement_timeout = %s\", ('300000',))  # timeout in milliseconds\n",
    "\n",
    "\n",
    "        # start_time = timer()\n",
    "\n",
    "        print( \"entering try block \") \n",
    "        \n",
    "        try:           \n",
    "            for query in rewriting:\n",
    "                cur.execute(query)\n",
    "        except Exception as e:\n",
    "            print(\"here\")\n",
    "            print(e)\n",
    "            print(\"start commit\")\n",
    "            conn.commit()\n",
    "            print(\"end commit\")\n",
    "\n",
    "\n",
    "        print(\"executing drop stuff\")\n",
    "        for drop in reversed(drop_statements):\n",
    "            cur.execute(drop)\n",
    "        print(\"fionished drop stuff\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    # return elapsed_time\n",
    "\n",
    "    \n",
    "    def run_query(self, query):\n",
    "        conn = psycopg2.connect(\n",
    "            host=self.host,\n",
    "            database = self.database,\n",
    "            user = self.user,\n",
    "            password = self.password\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "            \n",
    "        cur.execute(\"SET statement_timeout = %s\", ('300000',))  # timeout in milliseconds\n",
    "        \n",
    "        import time\n",
    "\n",
    "        start_time = timer()            \n",
    "    \n",
    "        try:\n",
    "            cur.execute(query)\n",
    "        except Exception as e:\n",
    "            print(\"here\")\n",
    "            print(e)\n",
    "            print(\"start commit\")\n",
    "            conn.commit()\n",
    "            print(\"end commit\")\n",
    "\n",
    "        elapsed_time = timer() - start_time\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return elapsed_time\n",
    "        \n",
    "    def close(self):\n",
    "        self.rewrite_process.kill()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# TODO: use this one for the unoptmised version\n",
    "\n",
    "class QueryRewriterUnOpt:\n",
    "    def __init__(self, host, database, user, password, port=5432, start_process=True):\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.port = port\n",
    "        self.jdbcString = f'jdbc:postgresql://{self.host}:{self.port}/{self.database}'\n",
    "        self.node_to_cost = dict()        \n",
    "        self.node_to_cost_ideal = dict()\n",
    "\n",
    "        if start_process:\n",
    "            self.rewrite_process = subprocess.Popen(['java', '-jar', REWRITE_JAR], stdout=subprocess.PIPE)\n",
    "            # Wait for the first line which is printed after the py4j server is started)\n",
    "            line = self.rewrite_process.stdout.readline()\n",
    "            # print(line)\n",
    "\n",
    "        self.gateway = JavaGateway()\n",
    "\n",
    "        self.rewriter = self.gateway.entry_point\n",
    "        self.rewriter.connect(self.jdbcString, self.database, self.user, self.password)\n",
    "\n",
    "\n",
    "    def rewrite_check_soft_numbers(self, query, k = 2, topn = 1):\n",
    "        SizeSoftSet = 0\n",
    "        \n",
    "        self.rewriter.connect(self.jdbcString, self.database, self.user, self.password)\n",
    "        self.rewriter.rewrite(query)\n",
    "\n",
    "        output = json.loads(Path('output/output.json').read_text())\n",
    "        drop_output = json.loads(Path('output/drop.json').read_text())\n",
    "\n",
    "        result = Rewriting(query, output['rewritten_query'], output['features'], output['time'],\n",
    "                           drop_output['rewritten_query'])\n",
    "\n",
    "        hg = HyperGraph.fromHyperbench('output/hypergraph.txt')\n",
    "\n",
    "        acyclic = output['acyclic']\n",
    "\n",
    "        if acyclic == True:\n",
    "            print(\"query is acyclic. done\")\n",
    "            for line in output[\"rewritten_query\"]:\n",
    "                print(line + \";\")\n",
    "            return \"acyclic\"\n",
    "        \n",
    "        # print('hg: ' + str(hg.markComplete() ) )\n",
    "        # hg = hg.markComplete()\n",
    "        # ctd = CTDOpt(hg)\n",
    "        # print('ctd: ' + str(ctd))\n",
    "        blocksAll = computesoftkBlocks(hg,k)\n",
    "        #blocks = computesoftkBlocksConnected(hg,k)\n",
    "        blocks = computesoftkBlocksConstraint(hg,k, lambda b: b.connected())\n",
    "\n",
    "        allDict = dict()\n",
    "\n",
    "        for b in blocksAll: \n",
    "            if b.head in allDict:\n",
    "                continue\n",
    "            allDict[b.head] = 0\n",
    "\n",
    "        conDict = dict()\n",
    "\n",
    "        for b in blocks: \n",
    "            if b.head in conDict:\n",
    "                continue\n",
    "            conDict[b.head] = 0\n",
    "        \n",
    "       \n",
    "        return (len(allDict.keys()),len(conDict.keys()),len(hg.E))\n",
    "\n",
    "    def rewrite(self, query, k = 2, topn = 1, Connected=False):\n",
    "        \n",
    "        self.rewriter.connect(self.jdbcString, self.database, self.user, self.password)\n",
    "        self.rewriter.rewrite(query)\n",
    "\n",
    "        output = json.loads(Path('output/output.json').read_text())\n",
    "        drop_output = json.loads(Path('output/drop.json').read_text())\n",
    "\n",
    "        result = Rewriting(query, output['rewritten_query'], output['features'], output['time'],\n",
    "                           drop_output['rewritten_query'])\n",
    "\n",
    "        hg = HyperGraph.fromHyperbench('output/hypergraph.txt')\n",
    "\n",
    "        acyclic = output['acyclic']\n",
    "\n",
    "        if acyclic == True:\n",
    "            print(\"query is acyclic. done\")\n",
    "            for line in output[\"rewritten_query\"]:\n",
    "                print(line + \";\")\n",
    "            return \"acyclic\"\n",
    "        \n",
    "        print('hg: ' + str(hg.markComplete() ) )\n",
    "        hg = hg.markComplete()\n",
    "        ctd = CTDOpt(hg)\n",
    "\n",
    "        if Connected:\n",
    "            blocks = computesoftkBlocksConstraint(hg,k, lambda b: b.connected())\n",
    "        else:\n",
    "            blocks = computesoftkBlocks(hg,k)\n",
    "        \n",
    "        # keep only distinct covers\n",
    "        covers_dict = dict()\n",
    "        for b in blocks:\n",
    "            cover = list(map(lambda e: e.name, b.cover))\n",
    "            index = \",\".join(sorted(cover))\n",
    "            # ignore single edges\n",
    "            if not index in covers_dict:\n",
    "                covers_dict[index] = b.cover\n",
    "\n",
    "        # Get non-single-edge covers\n",
    "        candidate_covers = [[e.name for e in cover] for cover in covers_dict.values() if len(cover) > 1]\n",
    "        single_edge_covers = [[e.name] for e in hg.E if e not in candidate_covers]\n",
    "        candidate_covers = candidate_covers + single_edge_covers\n",
    "\n",
    "        for cover in candidate_covers:\n",
    "            index = \",\".join(sorted(cover))\n",
    "            self.node_to_cost[index] = 0\n",
    "            self.node_to_cost_ideal[index] = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        # print(\"candidate covers\", candidate_covers)\n",
    "        # node_explain_plans = json.loads(self.rewriter.determineNodeWeightsJSON(json.dumps(candidate_covers)))\n",
    "       \n",
    "        # for (cover, explain) in node_explain_plans:\n",
    "        #     index = \",\".join(sorted(cover))\n",
    "        #     # plan = json.loads(explain)[0]\n",
    "        #     # cost = plan['Plan']['Plan Rows']\n",
    "        #     cost = int(explain)\n",
    "        #     self.node_to_cost[index] = cost\n",
    "        # print(\"node costs: \" + str(self.node_to_cost))\n",
    "\n",
    "        # node_explain_plans_ideal = json.loads(self.rewriter.determineNodeWeightsJSONIdeal(json.dumps(candidate_covers)))\n",
    "       \n",
    "        # for (cover, explain) in node_explain_plans_ideal:\n",
    "        #     index = \",\".join(sorted(cover))\n",
    "        #     plan = json.loads(explain)[0]\n",
    "        #     cost = plan['Plan']['Plan Rows']\n",
    "        #     self.node_to_cost_ideal[index] = cost\n",
    "        # print(\"node costs ideal: \" + str(self.node_to_cost_ideal))\n",
    "\n",
    "        semijoins = get_connected_semijoinsAll(covers_dict.values())\n",
    "        # keep only edge names (no vertices)\n",
    "        semijoins = [([e.name for e in cover1], [e.name for e in cover2]) for (cover1, cover2) in semijoins]\n",
    "\n",
    "        # print(\"Getting sj costs from Java\")\n",
    "        # sj_explain_plans = json.loads(self.rewriter.determineSemijoinWeightsJSON(json.dumps(semijoins)))\n",
    "        # print(\"Received sj costs from Java\")\n",
    "\n",
    "        \n",
    "        # sj_to_cost = dict()\n",
    "        # for (c1, c2, explain) in sj_explain_plans:\n",
    "        sj_to_cost = dict()\n",
    "        for (c1, c2) in semijoins:\n",
    "            # print(\"explain \",explain)\n",
    "            index1 = \",\".join(sorted(c1))\n",
    "            index2 = \",\".join(sorted(c2))\n",
    "            index = index1 + \"-\" + index2\n",
    "            # plan = json.loads(explain)[0]\n",
    "            # sj_cost = plan['Plan']['Total Cost']\n",
    "            # node_costs = self.node_to_cost_ideal.get(index1, 0) + self.node_to_cost_ideal.get(index2, 0)\n",
    "            # sj_to_cost[index] = max(sj_cost - node_costs, 1)\n",
    "            sj_to_cost[index] =0\n",
    "        # print(\"sj costs: \" + str(sj_to_cost))\n",
    "\n",
    "        #print(\"\\n\".join(map(lambda b: str(b), blocks)))\n",
    "        ctd.add_weights(self.node_to_cost)\n",
    "        ctd.add_weights_ideal(self.node_to_cost_ideal)\n",
    "        ctd.add_sj_weights(sj_to_cost)\n",
    "        blocks = get_best_blocks(blocks, self.node_to_cost)\n",
    "        \n",
    "\n",
    "        print(\"start to ad blocks\")\n",
    "        \n",
    "        # print(\"\\n\".join(map(lambda b: str(b), blocks)))\n",
    "        # The weights have to be added before the blocks because the join costs for trivially satisfied blocks\n",
    "        # are set in addBlock\n",
    "        for b in blocks:\n",
    "            ctd.addBlock(b)\n",
    "\n",
    "        print(\"added all blocks \", len(blocks))\n",
    "        \n",
    "        \n",
    "        res = ctd.minimize_weights(topn)\n",
    "        # print(\"Result: \",res)\n",
    "\n",
    "        rewritings = []\n",
    "        drops = []\n",
    "        tds = []\n",
    "\n",
    "        for td in res:\n",
    "            # print(\"---------------\\nTD as JSON\\n------------------\\n\")\n",
    "            # print(json.dumps(td, cls=NodeEncoder))\n",
    "\n",
    "            # print(\"original TD \", td)\n",
    "            td.removeMarkers()\n",
    "            # print(\"Cleaned Up TD: \",td)\n",
    "            \n",
    "            output = self.rewriter.rewriteCyclicJSON(json.dumps(td, cls=NodeEncoder))\n",
    "            output = json.loads(Path('output/output.json').read_text())\n",
    "            # print(\"-------------\\n OUTPUTTT \\n -----------\")\n",
    "            # print(output)\n",
    "            rewritings.append(output[\"rewritten_query\"])\n",
    "            # for line in output[\"rewritten_query\"]:\n",
    "            #     print(line + \";\")\n",
    "            drop_output = json.loads(Path('output/drop.json').read_text())\n",
    "            drops.append(drop_output[\"rewritten_query\"])\n",
    "            tds.append(td)\n",
    "\n",
    "        return (rewritings, drops, tds)\n",
    "\n",
    "    def run_rewriting(self, rewriting, drop_statements):\n",
    "            \n",
    "        conn = psycopg2.connect(\n",
    "            host=self.host,\n",
    "            database = self.database,\n",
    "            user = self.user,\n",
    "            password = self.password\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "                    \n",
    "        cur.execute(\"SET statement_timeout = %s\", ('300000',))  # timeout in milliseconds\n",
    "\n",
    "\n",
    "        # start_time = timer()\n",
    "\n",
    "        print( \"entering try block \") \n",
    "        \n",
    "        try:           \n",
    "            for query in rewriting:\n",
    "                cur.execute(query)\n",
    "        except Exception as e:\n",
    "            print(\"here\")\n",
    "            print(e)\n",
    "            print(\"start commit\")\n",
    "            conn.commit()\n",
    "            print(\"end commit\")\n",
    "\n",
    "\n",
    "        print(\"executing drop stuff\")\n",
    "        for drop in reversed(drop_statements):\n",
    "            cur.execute(drop)\n",
    "        print(\"fionished drop stuff\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    # return elapsed_time\n",
    "\n",
    "    \n",
    "    def run_query(self, query):\n",
    "        conn = psycopg2.connect(\n",
    "            host=self.host,\n",
    "            database = self.database,\n",
    "            user = self.user,\n",
    "            password = self.password\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "            \n",
    "        cur.execute(\"SET statement_timeout = %s\", ('300000',))  # timeout in milliseconds\n",
    "        \n",
    "        import time\n",
    "\n",
    "        start_time = timer()            \n",
    "    \n",
    "        try:\n",
    "            cur.execute(query)\n",
    "        except Exception as e:\n",
    "            print(\"here\")\n",
    "            print(e)\n",
    "            print(\"start commit\")\n",
    "            conn.commit()\n",
    "            print(\"end commit\")\n",
    "\n",
    "        elapsed_time = timer() - start_time\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return elapsed_time\n",
    "        \n",
    "    def close(self):\n",
    "        self.rewrite_process.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dce1e4-bb4d-48f7-b796-6ced6de1ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "\n",
    "def run_top_rewritings(rewriter, query, k = 2, topn = 1,name=\"\"):\n",
    "    (rewritings, drops, tds) = rewriter.rewrite(query, k, topn)\n",
    "\n",
    "    results = []\n",
    "    for i, rewriting in enumerate(rewritings):\n",
    "        drop_statements = drops[i]\n",
    "        td = tds[i]\n",
    "        print(\"running query\", i)\n",
    "\n",
    "       \n",
    "        times = []\n",
    "        for x in range(0, 3):  \n",
    "            start_time = timer()\n",
    "            rewriter.run_rewriting(rewriting, drop_statements)\n",
    "            elapsed_time = timer() - start_time\n",
    "            times.append(elapsed_time)\n",
    "        \n",
    "        print(\"Times list: \",times)\n",
    "         \n",
    "        rewriterJoin = \";\\n\".join(rewriting)\n",
    "        results.append([\"TD\" + str(i),statistics.mean(times), td.weight,td.weight_ideal,td.toStringCost(1,rewriter.node_to_cost),rewriterJoin])\n",
    "        # print(results)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns = ['td_name','runtime', 'cost', 'cost_ideal', 'decomp', 'rewriting' ])\n",
    "    df.to_csv('results'+name+str(datetime.now().strftime(\"%d%m%Y%H%M%S\"))+'.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_top_rewritings_soft_nums(rewriter, query, k = 2, topn = 1,name=\"\"):\n",
    "    return rewriter.rewrite_check_soft_numbers(query,k,topn)\n",
    "\n",
    "\n",
    "def run_top_rewritings_check_TD_comp_time(rewriter, query, k = 2, topn = 1,name=\"\"):\n",
    "    rewriter.rewrite(query, k, topn, quitAfterComputingTDs = True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52de11-2b05-48a2-ad62-a95a877f0af6",
   "metadata": {},
   "source": [
    "Below begin the tests for the 6 queries. For each, there are two cells: \n",
    "* The first cell runs the experiment. There are a number of options to select the kind of experiment to run\n",
    "* The second cell visualises the results, plotting the runtime of each TD against its cost and the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebe88f-d61b-494a-8048-6e4ad486345e",
   "metadata": {},
   "source": [
    "## Q_TPC-DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7943285-8b62-475b-8935-30e84e1bbcbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL TPS-DS Test\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "if 'rewriter' in locals():\n",
    "    rewriter.close()\n",
    "\n",
    "\n",
    "%reload_ext line_profiler\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT min(ws_bill_customer_sk)\n",
    "FROM   web_sales, \n",
    "       customer, \n",
    "       customer_address,\n",
    "       catalog_sales,\n",
    "       warehouse\n",
    "WHERE  ws_bill_customer_sk = c_customer_sk \n",
    "       AND ca_address_sk =  c_current_addr_sk \n",
    "       AND c_current_addr_sk = cs_bill_addr_sk\n",
    "       AND cs_warehouse_sk = w_warehouse_sk\n",
    "       AND  w_warehouse_sq_ft = ws_quantity\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "noOpt = True\n",
    "\n",
    "\n",
    "if noOpt:\n",
    "    rewriter = QueryRewriterUnOpt('localhost', 'tpcds', 'postgres', 'postgres')\n",
    "else:        \n",
    "    rewriter = QueryRewriter('localhost', 'tpcds', 'postgres', 'postgres')\n",
    "    \n",
    "\n",
    "\n",
    "size_check = False\n",
    "Connected = True\n",
    "OnlyComputeTDs = False\n",
    "\n",
    "\n",
    "if size_check:\n",
    "    sizeAll, sizeConnected,hgSize = run_top_rewritings_soft_nums(rewriter, query, k = 2, topn = 10,name=\"TPCDS-sortedTDs-filteredRoot\")\n",
    "\n",
    "    print(\"HG size: \", hgSize)\n",
    "    print(\"Size All:\", sizeAll)\n",
    "    print(\"Size Connected:\", sizeConnected)\n",
    "    \n",
    "else:\n",
    "    if OnlyComputeTDs:\n",
    "        run_top_rewritings_check_TD_comp_time(rewriter, query, k = 2, topn = 10,name=\"TPCDS-sortedTDs-filteredRoot\")\n",
    "    else:   \n",
    "        start_time = timer()\n",
    "        df_tpds = run_top_rewritings(rewriter, query, k = 2, topn = 10,name=\"TPCDS-unOpt-connected\",Connected)\n",
    "        run_time = timer() - start_time\n",
    "        print(\"run time Total: \", run_time)\n",
    "        \n",
    "        print(\"Comparison to baseline\")\n",
    "        time = rewriter.run_query(query)\n",
    "        print(\"Run time: \", time)\n",
    "\n",
    "\n",
    "\n",
    "rewriter.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703dbc5-02ae-4b94-ac01-e5caf117cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(x=df_tpcds['cost'],y=df_tpcds['runtime'])\n",
    "\n",
    "\n",
    "df_tpcds_proj = df_tpcds.loc[:,['cost', 'runtime']]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1.5, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = df_tpcds_proj.values, rowLabels = df_tpcds_proj.index, bbox=bbox, colLabels=df_tpcds_proj.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "\n",
    "\n",
    "df_tpcds_proj2 = df_tpcds.loc[:,['cost', 'cost_ideal', 'runtime']]\n",
    "\n",
    "df_tpcds_proj2['baseline']=time\n",
    "\n",
    "print(df_tpcds_proj2.to_csv(index=False,float_format='%.2f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56027449-5b50-4557-9233-86cc3cdf61f9",
   "metadata": {},
   "source": [
    "## Q_HTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fe1bf-1f2d-4243-8d17-46f0357e6b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL HETIO Test\n",
    "\n",
    "\n",
    "if 'rewriter' in locals():\n",
    "    rewriter.close()\n",
    "\n",
    "%reload_ext line_profiler\n",
    "\n",
    "query = \"\"\"\n",
    "select min(hetio45173_0.s)\n",
    "from   hetio45173 hetio45173_0, hetio45173 hetio45173_1, \n",
    "       hetio45160 hetio45160_2, hetio45160 hetio45160_3, \n",
    "       hetio45160 hetio45160_4, hetio45159 hetio45159_5, \n",
    "       hetio45159 hetio45159_6 \n",
    "where  hetio45173_0.s = hetio45173_1.s and hetio45173_0.d = hetio45160_2.s and \n",
    "       hetio45173_1.d = hetio45160_3.s and hetio45160_2.d = hetio45160_3.d and \n",
    "       hetio45160_3.d = hetio45160_4.s and hetio45160_4.s = hetio45159_5.s and \n",
    "       hetio45160_4.d = hetio45159_6.s and hetio45159_5.d = hetio45159_6.d\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "noOpt = True\n",
    "\n",
    "\n",
    "if noOpt:\n",
    "    rewriter = QueryRewriterUnOpt('localhost', 'hetio', 'postgres', 'postgres')\n",
    "else:        \n",
    "    rewriter = QueryRewriter('localhost', 'hetio', 'postgres', 'postgres')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "size_check = False\n",
    "Connected = True\n",
    "OnlyComputeTDs = False\n",
    "\n",
    "\n",
    "\n",
    "if size_check:\n",
    "    sizeAll, sizeConnected,hgSize = run_top_rewritings_soft_nums(rewriter, query, k = 2, topn = 10,name=\"HETIO-sortedTDs-filteredRoot\")\n",
    "\n",
    "    print(\"HG size: \", hgSize)\n",
    "    print(\"Size All:\", sizeAll)\n",
    "    print(\"Size Connected:\", sizeConnected)\n",
    "    \n",
    "else:\n",
    "    if OnlyComputeTDs:\n",
    "        run_top_rewritings_check_TD_comp_time(rewriter, query, k = 2, topn = 10,name=\"HETIO-sortedTDs-filteredRoot\")\n",
    "    else:   \n",
    "        start_time = timer()\n",
    "        df_hetio1 = run_top_rewritings(rewriter, query, k = 2, topn = 10,name=\"HETIO-unOpt-connected\", Connected)\n",
    "        run_time = timer() - start_time\n",
    "        print(\"run time Total: \", run_time)\n",
    "        \n",
    "                \n",
    "        print(\"Comparison to baseline\")\n",
    "        time = rewriter.run_query(query)\n",
    "        print(\"Run time: \", time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rewriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142d582-57c8-4112-8a44-47c05601a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(x=df_hetio1['cost'],y=df_hetio1['runtime'])\n",
    "\n",
    "\n",
    "df_hetio1_proj = df_hetio1.loc[:,['cost', 'runtime']]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1.5, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = df_hetio1_proj.values, rowLabels = df_hetio1_proj.index, bbox=bbox, colLabels=df_hetio1_proj.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "\n",
    "\n",
    "df_hetio1_proj2 = df_hetio1.loc[:,['cost', 'cost_ideal', 'runtime']]\n",
    "\n",
    "# df_hetio1_proj2['baseline']=time\n",
    "\n",
    "print(df_hetio1_proj2.to_csv(index=False,float_format='%.2f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125da27f-8bc0-49d8-814a-99b6dbc05410",
   "metadata": {},
   "source": [
    "## Q2_HTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf19adf-0d53-4777-bd7a-c109e1b6a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL HETIO2 Test\n",
    "\n",
    "\n",
    "if 'rewriter' in locals():\n",
    "    rewriter.close()\n",
    "\n",
    "%reload_ext line_profiler\n",
    "\n",
    "query = \"\"\"\n",
    "select      max(hetio45160.d) \n",
    "from        hetio45173 hetio45173_0, hetio45173 hetio45173_1, hetio45173 hetio45173_2, \n",
    "            hetio45173 hetio45173_3, hetio45160, hetio45176 hetio45176_5, hetio45176 \n",
    "            hetio45176_6 \n",
    "where       hetio45173_0.s = hetio45173_1.s and hetio45173_0.d = hetio45173_2.s and \n",
    "            hetio45173_1.d = hetio45173_3.s and hetio45173_2.d = hetio45173_3.d and \n",
    "            hetio45173_3.d = hetio45160.s and hetio45160.s = hetio45176_5.s and \n",
    "            hetio45160.d = hetio45176_6.s and hetio45176_5.d = hetio45176_6.d\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "noOpt = True\n",
    "\n",
    "\n",
    "if noOpt:\n",
    "    #rewriter = QueryRewriter('postgres', 'tpch', 'tpch', 'tpch', start_process=False)\n",
    "    rewriter = QueryRewriterUnOpt('localhost', 'hetio', 'postgres', 'postgres')\n",
    "else:        \n",
    "    #rewriter = QueryRewriter('postgres', 'tpch', 'tpch', 'tpch', start_process=False)\n",
    "    rewriter = QueryRewriter('localhost', 'hetio', 'postgres', 'postgres')\n",
    "    \n",
    "\n",
    "\n",
    "size_check = False\n",
    "Connected = True\n",
    "OnlyComputeTDs = False\n",
    "\n",
    "\n",
    "\n",
    "if size_check:\n",
    "    sizeAll, sizeConnected,hgSize = run_top_rewritings_soft_nums(rewriter, query, k = 2, topn = 10,name=\"HETIO2-sortedTDs-fiterRoot\")\n",
    "\n",
    "    print(\"HG size: \", hgSize)\n",
    "    print(\"Size All:\", sizeAll)\n",
    "    print(\"Size Connected:\", sizeConnected)\n",
    "    \n",
    "else:\n",
    "    if OnlyComputeTDs:\n",
    "        run_top_rewritings_check_TD_comp_time(rewriter, query, k = 2, topn = 10,name=\"HETIO2-sortedTDs-fiterRoot\")\n",
    "    else:   \n",
    "        start_time = timer()\n",
    "        df_hetio2 = run_top_rewritings(rewriter, query, k = 2, topn = 10,name=\"HETIO2-unOpt-connected\", Connected)\n",
    "        run_time = timer() - start_time\n",
    "        print(\"run time Total: \", run_time)\n",
    "\n",
    "                \n",
    "        print(\"Comparison to baseline\")\n",
    "        time = rewriter.run_query(query)\n",
    "        print(\"Run time: \", time)\n",
    "\n",
    "\n",
    "\n",
    "rewriter.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7326ae-a7dd-4101-9e03-02bbaf62012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(x=df_hetio2['cost'],y=df_hetio2['runtime'])\n",
    "\n",
    "\n",
    "df_hetio2_proj = df_hetio2.loc[:,['cost', 'runtime']]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1.5, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = df_hetio2_proj.values, rowLabels = df_hetio2_proj.index, bbox=bbox, colLabels=df_hetio2_proj.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "\n",
    "\n",
    "df_hetio2_proj2 = df_hetio2.loc[:,['cost', 'cost_ideal', 'runtime']]\n",
    "\n",
    "df_hetio2_proj2['baseline']=time\n",
    "\n",
    "print(df_hetio2_proj2.to_csv(index=False,float_format='%.2f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab054d9-a7aa-4c2f-b8a9-4e86e82664dc",
   "metadata": {},
   "source": [
    "## Q3_HTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb42140-d2c1-4fd5-a9b8-247724b94791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL hetio3 Test\n",
    "\n",
    "\n",
    "if 'rewriter' in locals():\n",
    "    rewriter.close()\n",
    "\n",
    "%reload_ext line_profiler\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "select  min(hetio45173_2.d)\n",
    "from    hetio45173 hetio45173_0, hetio45173 hetio45173_1, hetio45173 \n",
    "        hetio45173_2, hetio45173 hetio45173_3 \n",
    "where   hetio45173_0.s = hetio45173_1.s and hetio45173_0.d = hetio45173_2.s \n",
    "        and hetio45173_1.d = hetio45173_3.d and hetio45173_2.d = hetio45173_3.s\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "noOpt = True\n",
    "\n",
    "\n",
    "if noOpt:\n",
    "    rewriter = QueryRewriterUnOpt('localhost', 'hetio', 'postgres', 'postgres')\n",
    "else:        \n",
    "    rewriter = QueryRewriter('localhost', 'hetio', 'postgres', 'postgres')\n",
    "    \n",
    "\n",
    "\n",
    "size_check = False\n",
    "Connected = True\n",
    "OnlyComputeTDs = False\n",
    "\n",
    "\n",
    "\n",
    "if size_check:\n",
    "    sizeAll, sizeConnected,hgSize = run_top_rewritings_soft_nums(rewriter, query, k = 2, topn = 10,name=\"HETIO3-sortedTDs-fiterRoot\")\n",
    "\n",
    "    print(\"HG size: \", hgSize)\n",
    "    print(\"Size All:\", sizeAll)\n",
    "    print(\"Size Connected:\", sizeConnected)\n",
    "    \n",
    "else:\n",
    "    if OnlyComputeTDs:\n",
    "        run_top_rewritings_check_TD_comp_time(rewriter, query, k = 2, topn = 10,name=\"HETIO3-sortedTDs-fiterRoot\")\n",
    "    else:   \n",
    "        start_time = timer()\n",
    "        df_hetio3 = run_top_rewritings(rewriter, query, k = 2, topn = 10,name=\"HETIO3-unOpt-connected\",Connected)\n",
    "        run_time = timer() - start_time\n",
    "        print(\"run time Total: \", run_time)\n",
    "\n",
    "                \n",
    "        print(\"Comparison to baseline\")\n",
    "        time = rewriter.run_query(query)\n",
    "        print(\"Run time: \", time)\n",
    "\n",
    "\n",
    "\n",
    "rewriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f35d76-3d5b-405d-8ed0-ca7825d9419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(x=df_hetio3['cost'],y=df_hetio3['runtime'])\n",
    "\n",
    "\n",
    "df_hetio3_proj = df_hetio3.loc[:,['cost', 'runtime']]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1.5, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = df_hetio3_proj.values, rowLabels = df_hetio3_proj.index, bbox=bbox, colLabels=df_hetio3_proj.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "\n",
    "\n",
    "df_hetio3_proj2 = df_hetio3.loc[:,['cost', 'cost_ideal', 'runtime']]\n",
    "\n",
    "df_hetio3_proj2['baseline']=time\n",
    "\n",
    "print(df_hetio3_proj2.to_csv(index=False,float_format='%.2f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f5dc7-7535-4a67-a4d1-463e8bb68163",
   "metadata": {},
   "source": [
    "## Q4_HTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5bf088-d823-4ed3-bc2b-0a4d7ca885c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL hetio4 Test\n",
    "\n",
    "\n",
    "if 'rewriter' in locals():\n",
    "    rewriter.close()\n",
    "\n",
    "%reload_ext line_profiler\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "select  min(hetio45160_0.s) \n",
    "from    hetio45160 hetio45160_0, hetio45160 hetio45160_1, \n",
    "        hetio45177, hetio45160 hetio45160_3, hetio45159 \n",
    "        hetio45159_4, hetio45159 hetio45159_5 \n",
    "where   hetio45160_0.s = hetio45160_1.s and hetio45160_0.d = hetio45177.s \n",
    "        and hetio45160_1.d = hetio45177.d and hetio45177.d = hetio45160_3.s \n",
    "        and hetio45160_3.s = hetio45159_4.s and hetio45160_3.d = hetio45159_5.s \n",
    "        and hetio45159_4.d = hetio45159_5.d\n",
    "\"\"\"\n",
    "noOpt = True\n",
    "\n",
    "\n",
    "if noOpt:\n",
    "    rewriter = QueryRewriterUnOpt('localhost', 'hetio', 'postgres', 'postgres')\n",
    "else:        \n",
    "    rewriter = QueryRewriter('localhost', 'hetio', 'postgres', 'postgres')\n",
    "    \n",
    "\n",
    "\n",
    "size_check = False\n",
    "Connected = True\n",
    "OnlyComputeTDs = False\n",
    "\n",
    "\n",
    "if size_check:\n",
    "    sizeAll, sizeConnected,hgSize = run_top_rewritings_soft_nums(rewriter, query, k = 2, topn = 10,name=\"HETIO4-sortedTDs-fiterRoot\")\n",
    "\n",
    "    print(\"HG size: \", hgSize)\n",
    "    print(\"Size All:\", sizeAll)\n",
    "    print(\"Size Connected:\", sizeConnected)\n",
    "    \n",
    "else:\n",
    "    if OnlyComputeTDs:\n",
    "        run_top_rewritings_check_TD_comp_time(rewriter, query, k = 2, topn = 10,name=\"HETIO4-sortedTDs-fiterRoot\")\n",
    "    else:   \n",
    "        start_time = timer()\n",
    "        df_hetio4 = run_top_rewritings(rewriter, query, k = 2, topn = 10,name=\"HETIO4-unOpt-connected\", Connected)\n",
    "        run_time = timer() - start_time\n",
    "        print(\"run time Total: \", run_time)\n",
    "\n",
    "                \n",
    "        print(\"Comparison to baseline\")\n",
    "        time = rewriter.run_query(query)\n",
    "        print(\"Run time: \", time)\n",
    "\n",
    "\n",
    "\n",
    "rewriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4c0e6-0db0-45cd-986a-12bb2a4c8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(x=df_hetio4['cost'],y=df_hetio4['runtime'])\n",
    "\n",
    "\n",
    "df_hetio4_proj = df_hetio4.loc[:,['cost', 'runtime']]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1.5, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = df_hetio4_proj.values, rowLabels = df_hetio4_proj.index, bbox=bbox, colLabels=df_hetio4_proj.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "\n",
    "\n",
    "df_hetio4_proj2 = df_hetio4.loc[:,['cost', 'cost_ideal', 'runtime']]\n",
    "\n",
    "\n",
    "df_hetio4_proj2['baseline']=time\n",
    "\n",
    "print(df_hetio4_proj2.to_csv(index=False,float_format='%.2f'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bee52-b47e-47a2-9c34-9382ed048af2",
   "metadata": {},
   "source": [
    "## Q_LSQB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf9a21-0a87-4878-a672-cb80a91f54fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL LSQB Test\n",
    "#\n",
    "\n",
    "if 'rewriter' in locals():\n",
    "    rewriter.close()\n",
    "\n",
    "\n",
    "%reload_ext line_profiler\n",
    "\n",
    "import time\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT MIN(pkp1.Person1Id)\n",
    "FROM City AS CityA\n",
    "JOIN City AS CityB\n",
    "  ON CityB.isPartOf_CountryId = CityA.isPartOf_CountryId\n",
    "JOIN City AS CityC\n",
    "  ON CityC.isPartOf_CountryId = CityA.isPartOf_CountryId\n",
    "JOIN Person AS PersonA\n",
    "  ON PersonA.isLocatedIn_CityId = CityA.CityId\n",
    "JOIN Person AS PersonB\n",
    "  ON PersonB.isLocatedIn_CityId = CityB.CityId\n",
    "JOIN Person_knows_Person AS pkp1\n",
    "  ON pkp1.Person1Id = PersonA.PersonId\n",
    " AND pkp1.Person2Id = PersonB.PersonId\n",
    "\"\"\"\n",
    "\n",
    "noOpt = True\n",
    "\n",
    "\n",
    "if noOpt:\n",
    "    #rewriter = QueryRewriter('postgres', 'tpch', 'tpch', 'tpch', start_process=False)\n",
    "    rewriter = QueryRewriterUnOpt('localhost', 'lsqb', 'postgres', 'postgres')\n",
    "else:        \n",
    "    #rewriter = QueryRewriter('postgres', 'tpch', 'tpch', 'tpch', start_process=False)\n",
    "    rewriter = QueryRewriter('localhost', 'lsqb', 'postgres', 'postgres')\n",
    "    \n",
    "\n",
    "\n",
    "size_check = False\n",
    "Connected = True\n",
    "OnlyComputeTDs = False\n",
    "\n",
    "\n",
    "if size_check:\n",
    "    sizeAll, sizeConnected,hgSize = run_top_rewritings_soft_nums(rewriter, query, k = 3, topn = 10,name=\"LSQB-sortedTDs-fiterRoot\")\n",
    "\n",
    "    print(\"HG size: \", hgSize)\n",
    "    print(\"Size All:\", sizeAll)\n",
    "    print(\"Size Connected:\", sizeConnected)\n",
    "    \n",
    "else:\n",
    "    if OnlyComputeTDs:\n",
    "        run_top_rewritings_check_TD_comp_time(rewriter, query, k = 3, topn = 10,name=\"LSQB-sortedTDs-fiterRoot\")\n",
    "    else:   \n",
    "        start_time = timer()\n",
    "        df_lsqb = run_top_rewritings(rewriter, query, k = 3, topn = 10,name=\"LSQB-unOpt-connected\", Connected)\n",
    "        run_time = timer() - start_time\n",
    "        print(\"run time Total: \", run_time)\n",
    "\n",
    "                \n",
    "        print(\"Comparison to baseline\")\n",
    "        time = rewriter.run_query(query)\n",
    "        print(\"Run time: \", time)\n",
    "\n",
    "\n",
    "\n",
    "rewriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6232f7-eaa1-4f13-83eb-fbe604976535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(x=df_lsqb['cost'],y=df_lsqb['runtime'])\n",
    "\n",
    "\n",
    "df_lsqb_proj = df_lsqb.loc[:,['cost', 'runtime']]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1.5, 1]\n",
    "ax2.axis('off')\n",
    "mpl_table = ax2.table(cellText = df_lsqb_proj.values, rowLabels = df_lsqb_proj.index, bbox=bbox, colLabels=df_lsqb_proj.columns)\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "\n",
    "\n",
    "df_lsqb_proj2 = df_lsqb.loc[:,['cost', 'cost_ideal', 'runtime']]\n",
    "\n",
    "df_lsqb_proj2['baseline']=time\n",
    "\n",
    "print(df_lsqb_proj2.to_csv(index=False,float_format='%.2f'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
